---
title: Assessment of the Public Health Danger and Economic Consequences of Severe
  Weather Events in the United States
author: "John Woods"
date: "07/02/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE)
library(tidyverse)
library(lubridate)
library(reshape2)
```

## Synopsis





## Introduction

Extreme weather events are a regular occurrence in the United States. In 2021 alone there were 20 'Billion-dollar disaster events' leading to 688 fatalities and causing $145 Billion in damage (NOAA, 2022). Being able to predict and better allocate resources to these events will enable the creation of data driven policies and protocols that could reduce this loss of life and economic damage and speed up recovery. The aim of this report is to provide actionable insight into these events and give recommendations on how resources could be prioritized based on them.


## Data Processing

This analysis will be using the US National Oceanic and Atmospheric Administration's (NOAA) storm database. The database tracks major storms and weather events across the United States, including when and where they occur along with estimates of any fatalities, injuries and property damage. The data can be downloaded from: https://d396qusza40orc.cloudfront.net/repdata%2Fdata%2FStormData.csv.bz2

We first read in the data which is comma delimited keeping the header data.

```{r cache = TRUE, message = FALSE}
storm_data <- read_csv("repdata_data_StormData.csv.bz2")
```
```{r}
dim(storm_data)
```
```{r}
head(storm_data)
```

The data contains observations of 902297 events consisting of 37 features for each. We aren't interested in all of these features so we'll drop those that are not relevant to our high level analysis. We'll also make the column names lower case so they are easier to type.

```{r}
names(storm_data) <- tolower(names(storm_data))
storm_data <- subset(storm_data, select = -c(bgn_time, time_zone, bgn_range, bgn_azi, bgn_locati,
                                             end_date, end_time, county_end, countyendn, 
                                             end_range, end_azi, end_locati, f, mag, length, width,
                                             stateoffic, zonenames, latitude_e, longitude_,
                                             remarks, refnum, latitude, longitude, wfo))
```

The values for property and crop damage consist of a value column and a column denoting if the value is in thousands/millions etc. We'll condense these columns to dollar amounts.

```{r}
table(storm_data['propdmgexp'])
```
We can safely assume B/M/K are billion/million/thousand however without documentation on the others the safest cause of action is to drop the rows containing them. 

```{r}
storm_data <- storm_data %>%
  mutate(propdmgexp = tolower(propdmgexp), cropdmgexp = tolower(cropdmgexp)) %>%
  filter(propdmgexp %in% c("b", "m", "k", NA) & cropdmgexp %in% c("b", "m", "k", NA)) %>%
  mutate(propdmgexp = case_when(propdmgexp == "k" ~ 1000,
                                propdmgexp == "m" ~ 1000000,
                                propdmgexp == "b" ~ 1000000000,
                                is.na(propdmgexp) ~ 1),
         cropdmgexp = case_when(cropdmgexp == "k" ~ 1000,
                                cropdmgexp == "m" ~ 1000000,
                                cropdmgexp == "b" ~ 1000000000,
                                is.na(cropdmgexp) ~ 1)) %>%
  mutate(propdmg = propdmg * propdmgexp, cropdmg = cropdmg * cropdmgexp) %>%
  subset(select = -c(propdmgexp, cropdmgexp))
```

Currently the date feature is a date time string set to midnight for each date. Let's isolate the date so it's easier to work with.

```{r}
storm_data <- storm_data %>%
  mutate(bgn_date = mdy(as.character(map(str_split(bgn_date, pattern = " "), 1))))
```

The dataset has 973 unique weather events. There are some errors in this feature including typos and values that aren't weather events, these need to be addressed before analysis. In addition due to a lack of standardization in the recording of the data many categories are worded slightly differently. 

```{r}

thunderstorm_winds <- c("thundeerstorm winds", "thunderestorm winds", "thunderstorm  winds", "thunderstorm w inds", "thunderstorm wind", "thunderstorm wins", "thunderstorms", "thunderstorms wind", "thunderstorms winds", "thunderstormw winds", "thunderstormwinds", "thunderstrom wind", "thunderstrom winds", "thundertorm winds", "thundertsorm wind", "thundestorm winds", "thunerstorm winds", "thunderstorm winds", "tstm wind", "tstm wnd")


storm_data <- storm_data %>%
  mutate(evtype = tolower(evtype)) %>%
  filter(!grepl("summary", evtype)) %>%
  filter(!evtype %in% c("?", "high", "none", "apache county")) %>%
  mutate(evtype = case_when(evtype %in% thunderstorm_winds ~ "thunderstorm wind",
                            grepl("forest fire", evtype) ~ "wildfires",
                            grepl("wildfire", evtype) ~ "wildfires",
                            grepl("wild fire", evtype) ~ "wildfires",
                            grepl("storm surge", evtype) ~ "storm surge",
                            grepl("tropical storm", evtype) ~ "tropical storm",
                            grepl("flash flood", evtype) ~ "flash flood",
                            grepl("rip current", evtype) ~ "rip current",
                            grepl("beach erosin", evtype) ~ "beach erosion",
                            grepl("hail", evtype) ~ "hail",
                            grepl("avalance", evtype) ~ "avalanche",
                            grepl("blizzard", evtype) ~ "blizzard",
                            grepl("thunderstorm wind", evtype) ~ "thunderstorm wind",
                            grepl("tstm wind", evtype) ~ "thunderstorm wind",
                            grepl("hurricane", evtype) ~ "hurricane",
                            TRUE ~ evtype))

```

Just by picking at the low hanging fruit the number of categories has been reduced to 622. We'll now do a more thorough EDA to assess what more can be done. Due to the event type imbalance in the dataset we'll use the average as our initial summary statistic. Lets look at the highest average loss of life first.

```{r}

events_summary <- storm_data %>%
  group_by(evtype) %>%
  summarise(count = n(), avg_fatalities = mean(fatalities), avg_injuries = mean(injuries), avg_propdmg = mean(propdmg), avg_cropdmg = mean(cropdmg))
events_summary[order(-events_summary$avg_fatalities),]
```
Many of these event categories have only a handful of events which is problematic.

```{r}
mean(head(events_summary$count, 50) < 3)
```
Half of the most deadly events have less than 3 occurrences. Looking at the data it's obvious many of these occur multiple times in the data but under different names, these are the high outliers. I looked at using word distances and clustering to condense them but due to similarity (eg "major flood", "minor flood") the category names are not a good candidate. An easy way to make the data more representative would be to only look at categories with larger numbers of occurrences. This makes the assumption that the observations of the same type are randomly distributed into different categories which might not be the case. We should bare this in mind if we're to carry out further statistical analysis. To make sure we still catch rare events lets look at those that occur at least 5 times in the dataset, lets also drop observations that are 0 in every column as they are of no interest to us.

```{r}
events_5 <- events_summary %>%
  filter(count >= 5, avg_fatalities > 0 | avg_injuries > 0 | avg_propdmg > 0 | avg_cropdmg > 0)
events_5[order(events_5$evtype),]
```
We've managed to go from 973 unique weather events to 157 whilst only reducing the number of observations by 0.12%! (1089)

Lets store these weather events in a new dataframe
```{r}
events <- storm_data %>%
  filter(evtype %in% events_5$evtype)
```


In the last step we realized that there's a large event frequency imbalance in this dataset. This might reflect reality as some weather events are more common than others. However It could also indicate problems with the collection or labeling of the underlying data, so we should check for any inconsistencies. lets look at the number of occurrences for each event.

```{r}
event_frequency <- events_5[order(-events_5$count),]
event_frequency
```
This seems a bit odd. We know events that didn't cause any damage are included so why are events like tornadoes and flash floods occurring more often than high wind and flood in the data. Lets plot 10 highest frequency over time to get more insight.

```{r}
frequency_data <- storm_data %>%
  filter(evtype %in% head(event_frequency$evtype, 10))


ggplot(data = frequency_data, aes(bgn_date, color = evtype)) +
  geom_histogram()
```
So the only highest frequency weather events that were being recorded in this database prior to 1993 were tornadoes, thunderstorm winds and hail. It seems very unlikely that these were the only events causing damage so it looks like we have a lot of missing data prior to 1993. Lets also check the less frequent events over this period.

```{r}
distinct_events <- events %>%
  filter(bgn_date >= mdy("01/01/89") & bgn_date < mdy("01/01/99")) %>%
  group_by(year(bgn_date)) %>%
  summarise(events = n_distinct(evtype))
distinct_events
```

looking at all the years it is the case that up until 1993 only 3 types of events were recorded. We'll drop years prior to 1993 so that we're only looking at years where the majority of events are being recorded (because of poor data entry formatting we expect the number of distinct event types to differ from year to year)

```{r}
events <- events %>%
  filter(bgn_date >= mdy("01/01/93"))
events
```

As mentioned earlier we have also seen that sometimes events that didn;t cause damage are recorded and sometimes they aren't. This makes using the average values problematic and so we will use only the totals under the assumption that all events that did cause damage were recorded.


## Exploratory Data Analysis

Note: I've been limited to 3 figures in this assignment therefore I won't be including any EDA plots

### Damage to human life

First we will look at events that cause the greatest loss of life and injury.

```{r}
pop_dmg <- events %>%
  group_by(evtype) %>%
  summarise(count = n(), fatalities = sum(fatalities), injuries = sum(injuries))
pop_dmg[order(-pop_dmg$fatalities),]
```
```{r}
pop_dmg[order(-pop_dmg$injuries),]
```
The proportion of fatalities in the 10 most deadly types of events

```{r}
deadliest_events <- head(pop_dmg[order(-pop_dmg$fatalities),], 10)
sum(deadliest_events$fatalities) / sum(pop_dmg$fatalities)
```

Due to the complicated nature of measuring death against injury and the lack of information about the kinds of injuries sustained I'll be using fatalities alone when assessing how harmful to population health an event is. More specifically I will assess the 10 events with the highest fatality rate as they are responsible for 77% of the total fatalities attributed to weather events. The rationale behind this is that these are the types relevant to this specific analysis and only using them will improve clarity.

Lets make a data frame containing all observations from these types:
```{r}
pop_dmg_data <- events %>%
  filter(evtype %in% deadliest_events$evtype)
```

### Economic Damage

Whilst one could rightly argue that the loss of human life in itself is a form of economic damage we will only be regarding damage to property and crops in this part of the analysis.

As both property and crop damage are measured in dollar amounts we will simply combine them to address the economic damage of different types of events.

```{r}
economic_dmg <- events %>%
  group_by(evtype) %>%
  summarise(count = n(), damage = sum(propdmg + cropdmg))
economic_dmg[order(-economic_dmg$damage),]
```

We can see floods have had the greatest total economic consequences from 1993 - 2011 and different types of flooding occur several times in the 10 most damaging events. As with fatalities lets see what proportion of economic damage the top ten events account for:

```{r}
costliest_events <- head(economic_dmg[order(-economic_dmg$damage),], 10)
sum(costliest_events$damage) / sum(economic_dmg$damage)
```
The top ten events occur for 90% of the economic damage from all weather events - as with fatalities we'll only look at these events in relation to economic damage.

```{r}
economic_dmg_data <- events %>%
  filter(evtype %in% costliest_events$evtype)
```


## Analysis

To provide a simple understandable analysis I've decided to rely on the total fatalities and economic damage to assess which events have the greatest consequences to human life and the economy across the United States. The rationale for this decision is that the requirements of this analysis are to the damage of the event type as a whole and not individual occurunces of it.

The data spans approximately 60 years, a time period which has experienced the introduction new technologies (early warning systems, rescue equipment, etc), population movement and policy changes. Therefore it's important to analyze the temporal variations in order to account for the current situation.

```{r}
fatality_data <- storm_data %>%
  filter(evtype %in% fatalities_top_10$evtype)

economic_data <- storm_data %>%
  filter(evtype %in% economic_dmg_top_10$evtype)


ggplot(data = group_by(fatality_data, evtype), aes(bgn_date, log(fatalities), color = evtype)) +
  stat_summary(fun = "sum", geom = "line")
```

NOAA National Centers for Environmental Information (NCEI) U.S. Billion-Dollar Weather and Climate Disasters (2022). https://www.ncdc.noaa.gov/billions/, DOI: 10.25921/stkw-7w73